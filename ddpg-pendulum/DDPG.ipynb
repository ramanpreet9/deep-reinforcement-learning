{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients (DDPG)\n",
    "---\n",
    "In this notebook, we train DDPG with OpenAI Gym's Pendulum-v0 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(2)\n",
    "agent = Agent(state_size=3, action_size=1, random_seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking at  t=  199\n",
      "Episode 1\tAverage Score: -739.89breaking at  t=  199\n",
      "Episode 2\tAverage Score: -622.45breaking at  t=  199\n",
      "Episode 3\tAverage Score: -589.20breaking at  t=  199\n",
      "Episode 4\tAverage Score: -604.91breaking at  t=  199\n",
      "Episode 5\tAverage Score: -607.11breaking at  t=  199\n",
      "Episode 6\tAverage Score: -667.40breaking at  t=  199\n",
      "Episode 7\tAverage Score: -654.96breaking at  t=  199\n",
      "Episode 8\tAverage Score: -679.72breaking at  t=  199\n",
      "Episode 9\tAverage Score: -660.30breaking at  t=  199\n",
      "Episode 10\tAverage Score: -691.83breaking at  t=  199\n",
      "Episode 11\tAverage Score: -684.25breaking at  t=  199\n",
      "Episode 12\tAverage Score: -667.66breaking at  t=  199\n",
      "Episode 13\tAverage Score: -703.69breaking at  t=  199\n",
      "Episode 14\tAverage Score: -721.75breaking at  t=  199\n",
      "Episode 15\tAverage Score: -708.48breaking at  t=  199\n",
      "Episode 16\tAverage Score: -699.18breaking at  t=  199\n",
      "Episode 17\tAverage Score: -708.46breaking at  t=  199\n",
      "Episode 18\tAverage Score: -703.53breaking at  t=  199\n",
      "Episode 19\tAverage Score: -704.63breaking at  t=  199\n",
      "Episode 20\tAverage Score: -700.51breaking at  t=  199\n",
      "Episode 21\tAverage Score: -686.19breaking at  t=  199\n",
      "Episode 22\tAverage Score: -681.58breaking at  t=  199\n",
      "Episode 23\tAverage Score: -688.80breaking at  t=  199\n",
      "Episode 24\tAverage Score: -685.84breaking at  t=  199\n",
      "Episode 25\tAverage Score: -693.25breaking at  t=  199\n",
      "Episode 26\tAverage Score: -685.95breaking at  t=  199\n",
      "Episode 27\tAverage Score: -681.41breaking at  t=  199\n",
      "Episode 28\tAverage Score: -679.23breaking at  t=  199\n",
      "Episode 29\tAverage Score: -680.80breaking at  t=  199\n",
      "Episode 30\tAverage Score: -674.99breaking at  t=  199\n",
      "Episode 31\tAverage Score: -669.48breaking at  t=  199\n",
      "Episode 32\tAverage Score: -667.79breaking at  t=  199\n",
      "Episode 33\tAverage Score: -676.86breaking at  t=  199\n",
      "Episode 34\tAverage Score: -678.58breaking at  t=  199\n",
      "Episode 35\tAverage Score: -673.86breaking at  t=  199\n",
      "Episode 36\tAverage Score: -675.74breaking at  t=  199\n",
      "Episode 37\tAverage Score: -671.09breaking at  t=  199\n",
      "Episode 38\tAverage Score: -675.69breaking at  t=  199\n",
      "Episode 39\tAverage Score: -676.24breaking at  t=  199\n",
      "Episode 40\tAverage Score: -677.21breaking at  t=  199\n",
      "Episode 41\tAverage Score: -672.97breaking at  t=  199\n",
      "Episode 42\tAverage Score: -668.93breaking at  t=  199\n",
      "Episode 43\tAverage Score: -665.55breaking at  t=  199\n",
      "Episode 44\tAverage Score: -672.44breaking at  t=  199\n",
      "Episode 45\tAverage Score: -676.50breaking at  t=  199\n",
      "Episode 46\tAverage Score: -683.46breaking at  t=  199\n",
      "Episode 47\tAverage Score: -688.72breaking at  t=  199\n",
      "Episode 48\tAverage Score: -692.04breaking at  t=  199\n",
      "Episode 49\tAverage Score: -690.03breaking at  t=  199\n",
      "Episode 50\tAverage Score: -690.81breaking at  t=  199\n",
      "Episode 51\tAverage Score: -687.13breaking at  t=  199\n",
      "Episode 52\tAverage Score: -685.78breaking at  t=  199\n",
      "Episode 53\tAverage Score: -682.36breaking at  t=  199\n",
      "Episode 54\tAverage Score: -678.95breaking at  t=  199\n",
      "Episode 55\tAverage Score: -675.71breaking at  t=  199\n",
      "Episode 56\tAverage Score: -678.74breaking at  t=  199\n",
      "Episode 57\tAverage Score: -675.62breaking at  t=  199\n",
      "Episode 58\tAverage Score: -672.67breaking at  t=  199\n",
      "Episode 59\tAverage Score: -675.38breaking at  t=  199\n",
      "Episode 60\tAverage Score: -678.21breaking at  t=  199\n",
      "Episode 61\tAverage Score: -681.03breaking at  t=  199\n",
      "Episode 62\tAverage Score: -686.59breaking at  t=  199\n",
      "Episode 63\tAverage Score: -689.55breaking at  t=  199\n",
      "Episode 64\tAverage Score: -691.71breaking at  t=  199\n",
      "Episode 65\tAverage Score: -692.13breaking at  t=  199\n",
      "Episode 66\tAverage Score: -694.66breaking at  t=  199\n",
      "Episode 67\tAverage Score: -691.46breaking at  t=  199\n",
      "Episode 68\tAverage Score: -690.33breaking at  t=  199\n",
      "Episode 69\tAverage Score: -687.37breaking at  t=  199\n",
      "Episode 70\tAverage Score: -689.77breaking at  t=  199\n",
      "Episode 71\tAverage Score: -687.13breaking at  t=  199\n",
      "Episode 72\tAverage Score: -684.53breaking at  t=  199\n",
      "Episode 73\tAverage Score: -685.20breaking at  t=  199\n",
      "Episode 74\tAverage Score: -682.94breaking at  t=  199\n",
      "Episode 75\tAverage Score: -682.00breaking at  t=  199\n",
      "Episode 76\tAverage Score: -679.36breaking at  t=  199\n",
      "Episode 77\tAverage Score: -677.15breaking at  t=  199\n",
      "Episode 78\tAverage Score: -679.60breaking at  t=  199\n",
      "Episode 79\tAverage Score: -677.08breaking at  t=  199\n",
      "Episode 80\tAverage Score: -674.91breaking at  t=  199\n",
      "Episode 81\tAverage Score: -674.20breaking at  t=  199\n",
      "Episode 82\tAverage Score: -677.08breaking at  t=  199\n",
      "Episode 83\tAverage Score: -675.08breaking at  t=  199\n",
      "Episode 84\tAverage Score: -678.73breaking at  t=  199\n",
      "Episode 85\tAverage Score: -679.05breaking at  t=  199\n",
      "Episode 86\tAverage Score: -679.67breaking at  t=  199\n",
      "Episode 87\tAverage Score: -678.48breaking at  t=  199\n",
      "Episode 88\tAverage Score: -677.75breaking at  t=  199\n",
      "Episode 89\tAverage Score: -675.29breaking at  t=  199\n",
      "Episode 90\tAverage Score: -673.36breaking at  t=  199\n",
      "Episode 91\tAverage Score: -672.70breaking at  t=  199\n",
      "Episode 92\tAverage Score: -673.62breaking at  t=  199\n",
      "Episode 93\tAverage Score: -673.02breaking at  t=  199\n",
      "Episode 94\tAverage Score: -671.24breaking at  t=  199\n",
      "Episode 95\tAverage Score: -670.24breaking at  t=  199\n",
      "Episode 96\tAverage Score: -668.51breaking at  t=  199\n",
      "Episode 97\tAverage Score: -667.96breaking at  t=  199\n",
      "Episode 98\tAverage Score: -666.28breaking at  t=  199\n",
      "Episode 99\tAverage Score: -666.93breaking at  t=  199\n",
      "Episode 100\tAverage Score: -666.49\n",
      "breaking at  t=  199\n",
      "Episode 101\tAverage Score: -667.46breaking at  t=  199\n",
      "Episode 102\tAverage Score: -671.70"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6a2542dbdc7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-6a2542dbdc7c>\u001b[0m in \u001b[0;36mddpg\u001b[1;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Gdrive\\Work\\deep-reinforcement-learning\\ddpg-pendulum\\ddpg_agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, add_noise)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;34m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=1000, max_t=300, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                print('breaking at  t= ', t)\n",
    "                break \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(200):\n",
    "    action = agent.act(state, add_noise=False)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Explore\n",
    "\n",
    "In this exercise, we have provided a sample DDPG agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster than this benchmark implementation.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task!\n",
    "- Write your own DDPG implementation.  Use this code as reference only when needed -- try as much as you can to write your own algorithm from scratch.\n",
    "- You may also like to implement prioritized experience replay, to see if it speeds learning.  \n",
    "- The current implementation adds Ornsetein-Uhlenbeck noise to the action space.  However, it has [been shown](https://blog.openai.com/better-exploration-with-parameter-noise/) that adding noise to the parameters of the neural network policy can improve performance.  Make this change to the code, to verify it for yourself!\n",
    "- Write a blog post explaining the intuition behind the DDPG algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
